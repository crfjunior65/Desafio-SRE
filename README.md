# ğŸš€ Desafio SRE - Elvenworks

## ğŸ“‹ Sobre o Projeto

Projeto desenvolvido como parte do Nivelamento Tecnico e processo de InclusÃ£o na Equipe para a posiÃ§Ã£o de SRE / DevOps na Elvenworks. O desafio consiste em implementar uma stack completa de DevOps/SRE, desde a containerizaÃ§Ã£o de uma aplicaÃ§Ã£o atÃ© o deploy em Kubernetes com monitoramento completo.

---

## ğŸ¯ Objetivos do Desafio

### Primeira Semana
- âœ… Rodar aplicaÃ§Ã£o localmente
- âœ… Dockerizar aplicaÃ§Ã£o
- âœ… Provisionar com Terraform + Docker
- âœ… Deploy em Kubernetes local (Kind)
- âœ… Implementar monitoramento (Prometheus + Grafana)

### Segunda Semana
- âœ… Provisionar infraestrutura AWS (VPC, EKS, RDS, Kafka, Redis, OpenSearch)
- â³ CI/CD com ArgoCD
- â³ APM e coleta de mÃ©tricas
- â³ Logs centralizados no OpenSearch
- â³ OrganizaÃ§Ã£o de IaC
- â³ DocumentaÃ§Ã£o completa

---

## ğŸ“Š Resultados Obtidos

### âœ… Desafio 1 - AplicaÃ§Ã£o Local
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- AplicaÃ§Ã£o Flask rodando localmente
- ConexÃµes com PostgreSQL e Redis funcionando
- MÃ©tricas Prometheus expostas

**Tecnologias:**
- Python 3.12
- Flask 3.0.0
- PostgreSQL
- Redis
- Prometheus Flask Exporter

### âœ… Desafio 2 - DockerizaÃ§Ã£o
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- Dockerfile otimizado para produÃ§Ã£o
- Multi-stage build
- UsuÃ¡rio nÃ£o-root
- Health checks nativos
- Docker Compose para ambiente local

**Melhorias Aplicadas:**
- Gunicorn como servidor WSGI
- Workers e threads configurados
- Timeouts adequados
- Imagem slim (Python 3.12-slim)

### âœ… Desafio 3 - Terraform + Docker
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- Infraestrutura como cÃ³digo com Terraform
- Provisionamento de containers Docker
- Network isolada
- Volumes persistentes

**Recursos Criados:**
- Container Flask App
- Container PostgreSQL com volume
- Container Redis
- Network bridge customizada

**LocalizaÃ§Ã£o:** `terraform/PrimeiraSemana-Desafio-03/`

### âœ… Desafio 4 - Kubernetes com Kind
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- Cluster Kind com 1 control-plane
- NGINX Ingress Controller
- Namespace dedicado (desafio-sre)
- 3 rÃ©plicas da aplicaÃ§Ã£o Flask
- PostgreSQL com PVC
- Redis
- ConfigMaps e Secrets

**Recursos Kubernetes:**
```
NAMESPACE       RECURSO                 REPLICAS    STATUS
desafio-sre     flask-app               3/3         Running
desafio-sre     postgres                1/1         Running
desafio-sre     redis                   1/1         Running
```

**Funcionalidades:**
- Load balancing entre rÃ©plicas
- Health checks (liveness + readiness)
- Resource limits e requests
- Ingress para acesso externo
- Persistent storage para PostgreSQL

**LocalizaÃ§Ã£o:** `PrimeiraSemana-Desafio04/`

### âœ… Desafio 5 - Monitoramento
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- Prometheus Operator (kube-prometheus-stack)
- Grafana com dashboards
- ServiceMonitor para coleta de mÃ©tricas
- PrometheusRule com alertas
- MÃ©tricas da aplicaÃ§Ã£o integradas

**Stack de Monitoramento:**
- **Prometheus:** Coleta e armazenamento de mÃ©tricas
- **Grafana:** VisualizaÃ§Ã£o e dashboards
- **Kube State Metrics:** MÃ©tricas do cluster
- **Node Exporter:** MÃ©tricas dos nodes
- **Alertmanager:** Gerenciamento de alertas

**MÃ©tricas Coletadas:**
- RequisiÃ§Ãµes HTTP (total, taxa, duraÃ§Ã£o)
- Erros por status code
- Uso de CPU e memÃ³ria
- MÃ©tricas do Kubernetes
- MÃ©tricas customizadas da aplicaÃ§Ã£o

**Alertas Configurados:**
- FlaskAppDown: AplicaÃ§Ã£o offline >1min
- HighErrorRate: Taxa de erro >5% por 2min
- HighMemoryUsage: MemÃ³ria >80% por 5min

**Acesso:**
- Prometheus: `kubectl port-forward -n monitoring svc/prometheus-server-kube-prom-prometheus 9090:9090`
- Grafana: `kubectl port-forward -n monitoring svc/prometheus-server-grafana 3000:80`


---

## ğŸ—ï¸ Arquitetura Implementada

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Kind Cluster (Docker)                   â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         Namespace: desafio-sre                      â”‚ â”‚
â”‚  â”‚                                                     â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ â”‚
â”‚  â”‚  â”‚ Flask    â”‚  â”‚PostgreSQLâ”‚  â”‚  Redis   â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ (3 pods) â”‚  â”‚ (1 pod)  â”‚  â”‚ (1 pod)  â”‚        â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚ â”‚
â”‚  â”‚       â”‚             â”‚              â”‚              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”        â”‚ â”‚
â”‚  â”‚  â”‚         Services (ClusterIP)         â”‚        â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚         Namespace: monitoring                      â”‚â”‚
â”‚  â”‚                                                    â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚â”‚
â”‚  â”‚  â”‚Prometheusâ”‚  â”‚ Grafana  â”‚  â”‚AlertMgr  â”‚       â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚â”‚
â”‚  â”‚       â”‚                                           â”‚â”‚
â”‚  â”‚       â”‚ ServiceMonitor + PrometheusRule          â”‚â”‚
â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚         Namespace: ingress-nginx                   â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚  â”‚  â”‚      NGINX Ingress Controller                â”‚ â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tecnologias Utilizadas

### AplicaÃ§Ã£o
- **Python 3.12**
- **Flask 3.0.0** - Framework web
- **Gunicorn 21.2.0** - Servidor WSGI
- **PostgreSQL** - Banco de dados relacional
- **Redis** - Cache e sessÃµes
- **Prometheus Flask Exporter** - MÃ©tricas

### Infraestrutura
- **Docker** - ContainerizaÃ§Ã£o
- **Docker Compose** - OrquestraÃ§Ã£o local
- **Terraform** - Infrastructure as Code
- **Kind** - Kubernetes local
- **Kubernetes 1.32** - OrquestraÃ§Ã£o de containers

### Monitoramento
- **Prometheus** - Coleta de mÃ©tricas
- **Grafana** - VisualizaÃ§Ã£o
- **Alertmanager** - Alertas
- **Kube State Metrics** - MÃ©tricas K8s
- **Node Exporter** - MÃ©tricas de sistema

### CI/CD (Preparado)
- **GitHub Actions** - Pipeline CI/CD
- **ArgoCD** - GitOps deployment
- **Docker Hub** - Registry de imagens

---

## ğŸ“ Estrutura do Projeto

```
Desafio-SRE/
â”œâ”€â”€ app/                                    # AplicaÃ§Ã£o Flask
â”‚   â”œâ”€â”€ app.py                             # CÃ³digo da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ requirements.txt                   # DependÃªncias Python
â”‚   â””â”€â”€ Dockerfile                         # Imagem Docker otimizada
â”‚
â”œâ”€â”€ PrimeiraSemana-Desafio04/              # Kubernetes local
â”‚   â”œâ”€â”€ k8s/
â”‚   â”‚   â”œâ”€â”€ Deployments/                   # Manifests da aplicaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ Monitoring/                    # Manifests de monitoramento
â”‚   â”œâ”€â”€ limpar-cluster.sh                  # Script de limpeza
â”‚   â””â”€â”€ recriar-cluster.sh                 # Script de recriaÃ§Ã£o
â”‚
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ PrimeiraSemana-Desafio-03/         # Terraform + Docker
â”‚   â””â”€â”€ SegundaSemana/                     # Infraestrutura AWS
â”‚
â”œâ”€â”€ docker-compose.yaml                    # Ambiente local
â””â”€â”€ README.md                              # Este arquivo
```

---

## ğŸš€ Como Executar

### PrÃ©-requisitos
- Docker e Docker Compose
- Kind
- kubectl
- Helm
- Terraform (opcional)

### OpÃ§Ã£o 1: Cluster Completo (Recomendado)
```bash
cd PrimeiraSemana-Desafio04
./recriar-cluster.sh
```

**Tempo:** 10-15 minutos  
**Resultado:** Cluster 100% funcional com monitoramento

### OpÃ§Ã£o 2: Ambiente Local (Docker Compose)
```bash
docker-compose up -d
```

### OpÃ§Ã£o 3: Terraform + Docker
```bash
cd terraform/PrimeiraSemana-Desafio-03
terraform init
terraform apply
```

---

## ğŸ§ª Testes e ValidaÃ§Ã£o

### Testar AplicaÃ§Ã£o
```bash
# Port-forward
kubectl port-forward -n desafio-sre svc/flask-app-service 5000:80

# Endpoints
curl http://localhost:5000/              # Status
curl http://localhost:5000/health        # Health check
curl http://localhost:5000/redis         # Teste Redis
curl http://localhost:5000/postgres      # Teste PostgreSQL
curl http://localhost:5000/metrics       # MÃ©tricas Prometheus
```

### Acessar Monitoramento
```bash
# Prometheus
kubectl port-forward -n monitoring svc/prometheus-server-kube-prom-prometheus 9090:9090

# Grafana
kubectl port-forward -n monitoring svc/prometheus-server-grafana 3000:80
# http://localhost:3000 (admin/admin123)
```

---

## ğŸ“¸ EvidÃªncias Visuais

### Desafio 1 - AplicaÃ§Ã£o Local
![AplicaÃ§Ã£o Rodando Local](Imagens/PrimeiraSemana-Local-Rodando.png)
*AplicaÃ§Ã£o Flask rodando localmente com PostgreSQL e Redis*

![Teste de MÃ©tricas](Imagens/PrimeiraSemana-app-RodandoComPrometheus.png)
*MÃ©tricas Prometheus expostas na aplicaÃ§Ã£o*

### Desafio 2 - Docker Compose
![Docker Compose UP](Imagens/DockerCompose-UP.png)
*Containers rodando via Docker Compose*

![Docker Compose PS](Imagens/DockerComposePS.png)
*Status dos containers*

![Teste Redis](Imagens/curlCompose-redis.png)
*Teste de conexÃ£o com Redis*

### Desafio 3 - Terraform + Docker
![Terraform Apply](Imagens/Desafio-03-Concluido.png)
*Infraestrutura provisionada com Terraform*

![Terraform Docker](Imagens/TerraformDocker.png)
*Containers gerenciados pelo Terraform*

### Desafio 4 - Kubernetes Kind
![CriaÃ§Ã£o Cluster](Imagens/Desafio-04-CriacaoClusterKind.png)
*Cluster Kind sendo criado*

![Cluster UP](Imagens/ClusterKindUP.png)
*Cluster Kind funcionando*

![AtivaÃ§Ã£o Cluster](Imagens/Desafio04-AtivacaoCluster.png)
*Pods da aplicaÃ§Ã£o rodando*

![Carregando Imagem](Imagens/Desafio04-CarregandoImagemNoCluster-app-cluster.png)
*Imagem Docker sendo carregada no Kind*

### Desafio 5 - Monitoramento
![Helm List](Imagens/Desafio05-HelmList.png)
*Prometheus instalado via Helm*

![Pods Monitoring](Imagens/PodsMonitoring.png)
*Pods de monitoramento rodando*

![Prometheus OK](Imagens/Desafio05-Prometheus-OK.png)
*Prometheus coletando mÃ©tricas*

![Teste Prometheus](Imagens/Desafio05-TestePrometheus-OK.png)
*MÃ©tricas da aplicaÃ§Ã£o no Prometheus*

---

## ğŸ“ Aprendizados e Desafios

### Principais Aprendizados

1. **Kubernetes com Kind** - SimulaÃ§Ã£o de cluster local
2. **Observabilidade** - MÃ©tricas, alertas e dashboards
3. **Infrastructure as Code** - Terraform e manifests declarativos
4. **Boas PrÃ¡ticas** - Containers otimizados, health checks, secrets

### Desafios Enfrentados

1. **MÃ©tricas Prometheus** - Porta separada causando erros â†’ SoluÃ§Ã£o: mesma porta
2. **VersÃµes de DependÃªncias** - Flask incompatÃ­vel â†’ SoluÃ§Ã£o: atualizaÃ§Ã£o
3. **Health Checks** - Endpoint inadequado â†’ SoluÃ§Ã£o: `/health` dedicado
4. **Dockerfile** - Rodando como root â†’ SoluÃ§Ã£o: non-root + gunicorn

---

## ğŸ“Š EstatÃ­sticas do Projeto

- **Linhas de CÃ³digo:** ~500
- **Arquivos de ConfiguraÃ§Ã£o:** 30+
- **Containers:** 5
- **Namespaces Kubernetes:** 3
- **MÃ©tricas Coletadas:** 50+
- **Alertas Configurados:** 3
- **DocumentaÃ§Ã£o:** 100+ pÃ¡ginas

---
- [x] Provisionar infraestrutura AWS
---
### âœ… Desafio 6 - Infraestrutura AWS
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- Infraestrutura completa provisionada na AWS usando Terraform
- Arquitetura modular com remote state isolado por componente
- Multi-AZ para alta disponibilidade
- RegiÃ£o: us-east-2 (Ohio)

**Recursos Provisionados:**

**Networking:**
- VPC 10.100.0.0/16 com 2 Availability Zones (us-east-2a, us-east-2b)
- 2 Subnets pÃºblicas + 2 Subnets privadas
- 2 NAT Gateways para alta disponibilidade
- Internet Gateway
- Route Tables configuradas

**Compute:**
- EKS Cluster v1.34 com control plane gerenciado
- 3 Node Groups:
  - spot_1: 2 nodes t3.medium/t3a.medium (SPOT)
  - spot_2: 2 nodes t3.large/t3a.large (SPOT)
  - on_demand: 1 node t3.medium (ON_DEMAND)
- Auto-scaling configurado (5-11 nodes)
- IAM Roles com polÃ­ticas de menor privilÃ©gio

**Databases:**
- RDS PostgreSQL 17.6 (db.t3.micro)
- Multi-AZ para failover automÃ¡tico
- Backup automatizado
- Criptografia at-rest habilitada

**Cache:**
- ElastiCache Redis 7.0 (cache.t3.micro)
- ReplicaÃ§Ã£o Multi-AZ
- Automatic failover habilitado

**Messaging:**
- MSK (Managed Kafka) 3.5.1
- 2 brokers kafka.t3.small
- Multi-AZ deployment
- Criptografia in-transit e at-rest

**Search & Analytics:**
- OpenSearch 2.11 (t3.small.search)
- 2 nodes para alta disponibilidade
- Fine-grained access control
- Encryption at-rest e node-to-node

**Container Registry:**
- ECR (Elastic Container Registry)
- Image scanning habilitado
- Lifecycle policy (manter Ãºltimas 10 imagens)

**SeguranÃ§a:**
- Security Groups isolados por serviÃ§o
- Criptografia habilitada em todos os recursos
- IAM Roles com polÃ­ticas especÃ­ficas
- Secrets gerenciados pela AWS
- VPC endpoints para serviÃ§os AWS

**Observabilidade:**
- EKS Control Plane Logs habilitados
- CloudWatch Logs integrado
- Tags padronizadas em todos os recursos

**Infraestrutura como CÃ³digo:**
- 10 mÃ³dulos Terraform independentes
- Remote state no S3 com versionamento
- Backend isolado por mÃ³dulo
- VariÃ¡veis centralizadas
- Tags padrÃ£o aplicadas automaticamente

**MÃ³dulos Terraform:**
```
00-s3_remote_state/    # Bucket S3 para remote state
01-vpc/                # VPC + Subnets + NAT Gateways
02-security_group/     # Security Groups isolados
03-iam/                # IAM Roles para EKS
04-eks/                # EKS Cluster v1.34
05-node_groups/        # 3 Node Groups (2 SPOT + 1 ON_DEMAND)
06-rds/                # PostgreSQL Multi-AZ
07-kafka/              # MSK (Kafka) 2 brokers
08-redis/              # ElastiCache Redis replicado
09-opensearch/         # OpenSearch 2 nodes
10-ecr/                # Container Registry
```

**Desafios TÃ©cnicos Superados:**
1. **Capacidade SPOT:** InstÃ¢ncias t3a.medium indisponÃ­veis em us-east-1a â†’ MigraÃ§Ã£o para us-east-2 (us-east-2a, us-east-2b)
2. **Naming OpenSearch:** Domain name com underscores invÃ¡lido â†’ ImplementaÃ§Ã£o de funÃ§Ã£o replace() para sanitizaÃ§Ã£o
3. **Tags Padronizadas:** RepetiÃ§Ã£o manual de tags â†’ ImplementaÃ§Ã£o de default_tags no provider AWS
4. **ModularizaÃ§Ã£o:** DependÃªncias entre mÃ³dulos â†’ Remote state data sources para comunicaÃ§Ã£o

**Boas PrÃ¡ticas Implementadas:**
- âœ… Infraestrutura modular e reutilizÃ¡vel
- âœ… Remote state isolado por componente
- âœ… Multi-AZ em todos os serviÃ§os crÃ­ticos
- âœ… Criptografia por padrÃ£o
- âœ… Security Groups com menor privilÃ©gio
- âœ… Auto-scaling configurado
- âœ… Backup automatizado
- âœ… Tags consistentes para governanÃ§a
- âœ… DocumentaÃ§Ã£o inline nos mÃ³dulos

**Custo Estimado Mensal:**
- VPC (NAT Gateways): ~$65
- EKS Control Plane: ~$73
- EC2 Nodes (5-11 instances): ~$80-120
- RDS PostgreSQL: ~$30
- MSK (Kafka): ~$150
- ElastiCache Redis: ~$25
- OpenSearch: ~$80
- **Total:** ~$500-550/mÃªs

**LocalizaÃ§Ã£o:** `terraform/SegundaSemana/`

**Tempo de Provisionamento:** ~45 minutos

**ValidaÃ§Ã£o:**
```bash
# Configurar kubectl
aws eks update-kubeconfig --name desafio-sre-junior-eks --region us-east-2

# Verificar nodes
kubectl get nodes

# Verificar recursos AWS
aws eks describe-cluster --name desafio-sre-junior-eks --region us-east-2
aws rds describe-db-instances --region us-east-2
aws elasticache describe-cache-clusters --region us-east-2
```

---
## ğŸ”„ PrÃ³ximos Passos

### Segunda Semana
- [ ] Deploy com ArgoCD
- [ ] Implementar APM
- [ ] Centralizar logs no OpenSearch
- [ ] DocumentaÃ§Ã£o completa

---

---

## ğŸ‘¤ Autor

**Junior Fernandes**  
SRE / DevOps - ElvenWorks

---

**Ãšltima atualizaÃ§Ã£o:** 03/12/2025  
**VersÃ£o:** 1.1  
**Status:** Desafio 6 ConcluÃ­do âœ…
