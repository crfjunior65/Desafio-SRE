# ğŸš€ Desafio SRE - Elvenworks

## ğŸ“‹ Sobre o Projeto

Projeto desenvolvido como parte do Nivelamento Tecnico e processo de InclusÃ£o na Equipe para a posiÃ§Ã£o de SRE / DevOps na Elvenworks. O desafio consiste em implementar uma stack completa de DevOps/SRE, desde a containerizaÃ§Ã£o de uma aplicaÃ§Ã£o atÃ© o deploy em Kubernetes com monitoramento completo.

---

## ğŸ¯ Objetivos do Desafio

### Primeira Semana
- âœ… Rodar aplicaÃ§Ã£o localmente
- âœ… Dockerizar aplicaÃ§Ã£o
- âœ… Provisionar com Terraform + Docker
- âœ… Deploy em Kubernetes local (Kind)
- âœ… Implementar monitoramento (Prometheus + Grafana)

### Segunda Semana
- âœ… Provisionar infraestrutura AWS (VPC, EKS, RDS, Kafka, Redis, OpenSearch)
- âœ… CI/CD com ArgoCD
- â³ APM e coleta de mÃ©tricas
- â³ Logs centralizados no OpenSearch
- âœ… OrganizaÃ§Ã£o de IaC
- âœ… DocumentaÃ§Ã£o completa

---

## ğŸ“Š Resultados Obtidos

### âœ… Desafio 1 - AplicaÃ§Ã£o Local
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- AplicaÃ§Ã£o Flask rodando localmente
- ConexÃµes com PostgreSQL e Redis funcionando
- MÃ©tricas Prometheus expostas

**Tecnologias:**
- Python 3.12
- Flask 3.0.0
- PostgreSQL
- Redis
- Prometheus Flask Exporter

### âœ… Desafio 2 - DockerizaÃ§Ã£o
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- Dockerfile otimizado para produÃ§Ã£o
- Multi-stage build
- UsuÃ¡rio nÃ£o-root
- Health checks nativos
- Docker Compose para ambiente local

**Melhorias Aplicadas:**
- Gunicorn como servidor WSGI
- Workers e threads configurados
- Timeouts adequados
- Imagem slim (Python 3.12-slim)

### âœ… Desafio 3 - Terraform + Docker
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- Infraestrutura como cÃ³digo com Terraform
- Provisionamento de containers Docker
- Network isolada
- Volumes persistentes

**Recursos Criados:**
- Container Flask App
- Container PostgreSQL com volume
- Container Redis
- Network bridge customizada

**LocalizaÃ§Ã£o:** `terraform/PrimeiraSemana-Desafio-03/`

### âœ… Desafio 4 - Kubernetes com Kind
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- Cluster Kind com 1 control-plane
- NGINX Ingress Controller
- Namespace dedicado (desafio-sre)
- 3 rÃ©plicas da aplicaÃ§Ã£o Flask
- PostgreSQL com PVC
- Redis
- ConfigMaps e Secrets

**Recursos Kubernetes:**
```
NAMESPACE       RECURSO                 REPLICAS    STATUS
desafio-sre     flask-app               3/3         Running
desafio-sre     postgres                1/1         Running
desafio-sre     redis                   1/1         Running
```

**Funcionalidades:**
- Load balancing entre rÃ©plicas
- Health checks (liveness + readiness)
- Resource limits e requests
- Ingress para acesso externo
- Persistent storage para PostgreSQL

**LocalizaÃ§Ã£o:** `PrimeiraSemana-Desafio04/`

### âœ… Desafio 5 - Monitoramento
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- Prometheus Operator (kube-prometheus-stack)
- Grafana com dashboards
- ServiceMonitor para coleta de mÃ©tricas
- PrometheusRule com alertas
- MÃ©tricas da aplicaÃ§Ã£o integradas

**Stack de Monitoramento:**
- **Prometheus:** Coleta e armazenamento de mÃ©tricas
- **Grafana:** VisualizaÃ§Ã£o e dashboards
- **Kube State Metrics:** MÃ©tricas do cluster
- **Node Exporter:** MÃ©tricas dos nodes
- **Alertmanager:** Gerenciamento de alertas

**MÃ©tricas Coletadas:**
- RequisiÃ§Ãµes HTTP (total, taxa, duraÃ§Ã£o)
- Erros por status code
- Uso de CPU e memÃ³ria
- MÃ©tricas do Kubernetes
- MÃ©tricas customizadas da aplicaÃ§Ã£o

**Alertas Configurados:**
- FlaskAppDown: AplicaÃ§Ã£o offline >1min
- HighErrorRate: Taxa de erro >5% por 2min
- HighMemoryUsage: MemÃ³ria >80% por 5min

**Acesso:**
- Prometheus: `kubectl port-forward -n monitoring svc/prometheus-server-kube-prom-prometheus 9090:9090`
- Grafana: `kubectl port-forward -n monitoring svc/prometheus-server-grafana 3000:80`


---

## ğŸ—ï¸ Arquitetura Implementada

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Kind Cluster (Docker)                   â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         Namespace: desafio-sre                      â”‚ â”‚
â”‚  â”‚                                                     â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ â”‚
â”‚  â”‚  â”‚ Flask    â”‚  â”‚PostgreSQLâ”‚  â”‚  Redis   â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ (3 pods) â”‚  â”‚ (1 pod)  â”‚  â”‚ (1 pod)  â”‚        â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚ â”‚
â”‚  â”‚       â”‚             â”‚              â”‚              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”        â”‚ â”‚
â”‚  â”‚  â”‚         Services (ClusterIP)         â”‚        â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚         Namespace: monitoring                      â”‚â”‚
â”‚  â”‚                                                    â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚â”‚
â”‚  â”‚  â”‚Prometheusâ”‚  â”‚ Grafana  â”‚  â”‚AlertMgr  â”‚       â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚â”‚
â”‚  â”‚       â”‚                                           â”‚â”‚
â”‚  â”‚       â”‚ ServiceMonitor + PrometheusRule          â”‚â”‚
â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚         Namespace: ingress-nginx                   â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚  â”‚  â”‚      NGINX Ingress Controller                â”‚ â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Arquitetura AWS - Infraestrutura ProduÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          AWS Cloud - us-east-2 (Ohio)                        â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    VPC 10.100.0.0/16 (Multi-AZ)                        â”‚ â”‚
â”‚  â”‚                                                                        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ â”‚
â”‚  â”‚  â”‚   us-east-2a             â”‚  â”‚   us-east-2b             â”‚         â”‚ â”‚
â”‚  â”‚  â”‚                          â”‚  â”‚                          â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ Public Subnet      â”‚  â”‚  â”‚  â”‚ Public Subnet      â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ NAT Gateway        â”‚  â”‚  â”‚  â”‚ NAT Gateway        â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚           â”‚              â”‚  â”‚           â”‚             â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ Private Subnet     â”‚  â”‚  â”‚  â”‚ Private Subnet     â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                    â”‚  â”‚  â”‚  â”‚                    â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”‚ EKS Nodes      â”‚ â”‚  â”‚  â”‚  â”‚ â”‚ EKS Nodes      â”‚ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”‚ - SPOT (t3.*)  â”‚ â”‚  â”‚  â”‚  â”‚ â”‚ - SPOT (t3.*)  â”‚ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”‚ - ON_DEMAND    â”‚ â”‚  â”‚  â”‚  â”‚ â”‚ - ON_DEMAND    â”‚ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                    â”‚  â”‚  â”‚  â”‚                    â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”‚ RDS PostgreSQL â”‚ â”‚  â”‚  â”‚  â”‚ â”‚ RDS Standby    â”‚ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”‚ (Primary)      â”‚â—„â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â–ºâ”‚ (Multi-AZ)     â”‚ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                    â”‚  â”‚  â”‚  â”‚                    â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”‚ ElastiCache    â”‚ â”‚  â”‚  â”‚  â”‚ â”‚ ElastiCache    â”‚ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”‚ Redis (Primary)â”‚â—„â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â–ºâ”‚ Redis (Replica)â”‚ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                    â”‚  â”‚  â”‚  â”‚                    â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”‚ MSK Kafka      â”‚ â”‚  â”‚  â”‚  â”‚ â”‚ MSK Kafka      â”‚ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”‚ Broker 1       â”‚â—„â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â–ºâ”‚ Broker 2       â”‚ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                    â”‚  â”‚  â”‚  â”‚                    â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”‚ OpenSearch     â”‚ â”‚  â”‚  â”‚  â”‚ â”‚ OpenSearch     â”‚ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â”‚ Node 1         â”‚â—„â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â–ºâ”‚ Node 2         â”‚ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚         â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚
â”‚  â”‚                                                                        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚                    EKS Cluster v1.34                              â”‚ â”‚ â”‚
â”‚  â”‚  â”‚                                                                   â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ Namespace: desafio-sre                                       â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                                                              â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚ Flask Pod 1  â”‚  â”‚ Flask Pod 2  â”‚  â”‚ Flask Pod 3  â”‚     â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚         â”‚                 â”‚                 â”‚              â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                           â”‚                                 â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                  â”‚ LoadBalancer    â”‚                       â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                  â”‚ Service         â”‚                       â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚                              â”‚                                 â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ Namespace: argocd                                        â”‚  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                                                          â”‚  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚ ArgoCD       â”‚  â”‚ Repo Server  â”‚  â”‚ Application  â”‚  â”‚  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚ Server       â”‚  â”‚              â”‚  â”‚ Controller   â”‚  â”‚  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        ServiÃ§os Gerenciados                         â”‚  â”‚
â”‚  â”‚                                                                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚  â”‚  â”‚ ECR          â”‚  â”‚ CloudWatch   â”‚  â”‚ IAM Roles    â”‚            â”‚  â”‚
â”‚  â”‚  â”‚ (Registry)   â”‚  â”‚ (Logs/Metrics)â”‚  â”‚ (Security)   â”‚            â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                          CI/CD Pipeline                             â”‚  â”‚
â”‚  â”‚                                                                     â”‚  â”‚
â”‚  â”‚  GitHub â†’ GitHub Actions â†’ Docker Hub â†’ ArgoCD â†’ EKS              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legenda:
  â—„â”€â–º ReplicaÃ§Ã£o/Failover Multi-AZ
  â”‚   ComunicaÃ§Ã£o entre componentes
  â”Œâ”€â” Componente/ServiÃ§o
```

---

## ğŸ”„ PrÃ³ximos Passos

### AplicaÃ§Ã£o
- **Python 3.12**
- **Flask 3.0.0** - Framework web
- **Gunicorn 21.2.0** - Servidor WSGI
- **PostgreSQL** - Banco de dados relacional
- **Redis** - Cache e sessÃµes
- **Prometheus Flask Exporter** - MÃ©tricas

### Infraestrutura
- **Docker** - ContainerizaÃ§Ã£o
- **Docker Compose** - OrquestraÃ§Ã£o local
- **Terraform** - Infrastructure as Code
- **Kind** - Kubernetes local
- **Kubernetes 1.32** - OrquestraÃ§Ã£o de containers

### Monitoramento
- **Prometheus** - Coleta de mÃ©tricas
- **Grafana** - VisualizaÃ§Ã£o
- **Alertmanager** - Alertas
- **Kube State Metrics** - MÃ©tricas K8s
- **Node Exporter** - MÃ©tricas de sistema

### CI/CD (Preparado)
- **GitHub Actions** - Pipeline CI/CD
- **ArgoCD** - GitOps deployment
- **Docker Hub** - Registry de imagens

---

## ğŸ“ Estrutura do Projeto

```
Desafio-SRE/
â”œâ”€â”€ app/                                    # AplicaÃ§Ã£o Flask
â”‚   â”œâ”€â”€ app.py                             # CÃ³digo da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ requirements.txt                   # DependÃªncias Python
â”‚   â””â”€â”€ Dockerfile                         # Imagem Docker otimizada
â”‚
â”œâ”€â”€ PrimeiraSemana-Desafio04/              # Kubernetes local
â”‚   â”œâ”€â”€ k8s/
â”‚   â”‚   â”œâ”€â”€ Deployments/                   # Manifests da aplicaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ Monitoring/                    # Manifests de monitoramento
â”‚   â”œâ”€â”€ limpar-cluster.sh                  # Script de limpeza
â”‚   â””â”€â”€ recriar-cluster.sh                 # Script de recriaÃ§Ã£o
â”‚
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ PrimeiraSemana-Desafio-03/         # Terraform + Docker
â”‚   â””â”€â”€ SegundaSemana/                     # Infraestrutura AWS
â”‚
â”œâ”€â”€ docker-compose.yaml                    # Ambiente local
â””â”€â”€ README.md                              # Este arquivo
```

---

## ğŸš€ Como Executar

### PrÃ©-requisitos
- Docker e Docker Compose
- Kind
- kubectl
- Helm
- Terraform (opcional)

### OpÃ§Ã£o 1: Cluster Completo (Recomendado)
```bash
cd PrimeiraSemana-Desafio04
./recriar-cluster.sh
```

**Tempo:** 10-15 minutos  
**Resultado:** Cluster 100% funcional com monitoramento

### OpÃ§Ã£o 2: Ambiente Local (Docker Compose)
```bash
docker-compose up -d
```

### OpÃ§Ã£o 3: Terraform + Docker
```bash
cd terraform/PrimeiraSemana-Desafio-03
terraform init
terraform apply
```

---

## ğŸ§ª Testes e ValidaÃ§Ã£o

### Testar AplicaÃ§Ã£o
```bash
# Port-forward
kubectl port-forward -n desafio-sre svc/flask-app-service 5000:80

# Endpoints
curl http://localhost:5000/              # Status
curl http://localhost:5000/health        # Health check
curl http://localhost:5000/redis         # Teste Redis
curl http://localhost:5000/postgres      # Teste PostgreSQL
curl http://localhost:5000/metrics       # MÃ©tricas Prometheus
```

### Acessar Monitoramento
```bash
# Prometheus
kubectl port-forward -n monitoring svc/prometheus-server-kube-prom-prometheus 9090:9090

# Grafana
kubectl port-forward -n monitoring svc/prometheus-server-grafana 3000:80
# http://localhost:3000 (admin/admin123)
```

---

## ğŸ“¸ EvidÃªncias Visuais

### Desafio 1 - AplicaÃ§Ã£o Local
![AplicaÃ§Ã£o Rodando Local](Imagens/PrimeiraSemana-Local-Rodando.png)
*AplicaÃ§Ã£o Flask rodando localmente com PostgreSQL e Redis*

![Teste de MÃ©tricas](Imagens/PrimeiraSemana-app-RodandoComPrometheus.png)
*MÃ©tricas Prometheus expostas na aplicaÃ§Ã£o*

### Desafio 2 - Docker Compose
![Docker Compose UP](Imagens/DockerCompose-UP.png)
*Containers rodando via Docker Compose*

![Docker Compose PS](Imagens/DockerComposePS.png)
*Status dos containers*

![Teste Redis](Imagens/curlCompose-redis.png)
*Teste de conexÃ£o com Redis*

### Desafio 3 - Terraform + Docker
![Terraform Apply](Imagens/Desafio-03-Concluido.png)
*Infraestrutura provisionada com Terraform*

![Terraform Docker](Imagens/TerraformDocker.png)
*Containers gerenciados pelo Terraform*

### Desafio 4 - Kubernetes Kind
![CriaÃ§Ã£o Cluster](Imagens/Desafio-04-CriacaoClusterKind.png)
*Cluster Kind sendo criado*

![Cluster UP](Imagens/ClusterKindUP.png)
*Cluster Kind funcionando*

![AtivaÃ§Ã£o Cluster](Imagens/Desafio04-AtivacaoCluster.png)
*Pods da aplicaÃ§Ã£o rodando*

![Carregando Imagem](Imagens/Desafio04-CarregandoImagemNoCluster-app-cluster.png)
*Imagem Docker sendo carregada no Kind*

### Desafio 5 - Monitoramento
![Helm List](Imagens/Desafio05-HelmList.png)
*Prometheus instalado via Helm*

![Pods Monitoring](Imagens/PodsMonitoring.png)
*Pods de monitoramento rodando*

![Prometheus OK](Imagens/Desafio05-Prometheus-OK.png)
*Prometheus coletando mÃ©tricas*

![Teste Prometheus](Imagens/Desafio05-TestePrometheus-OK.png)
*MÃ©tricas da aplicaÃ§Ã£o no Prometheus*

---

## ğŸ“ Aprendizados e Desafios

### Principais Aprendizados

1. **Kubernetes com Kind** - SimulaÃ§Ã£o de cluster local
2. **Observabilidade** - MÃ©tricas, alertas e dashboards
3. **Infrastructure as Code** - Terraform e manifests declarativos
4. **Boas PrÃ¡ticas** - Containers otimizados, health checks, secrets

### Desafios Enfrentados

1. **MÃ©tricas Prometheus** - Porta separada causando erros â†’ SoluÃ§Ã£o: mesma porta
2. **VersÃµes de DependÃªncias** - Flask incompatÃ­vel â†’ SoluÃ§Ã£o: atualizaÃ§Ã£o
3. **Health Checks** - Endpoint inadequado â†’ SoluÃ§Ã£o: `/health` dedicado
4. **Dockerfile** - Rodando como root â†’ SoluÃ§Ã£o: non-root + gunicorn

---

## ğŸ“Š EstatÃ­sticas do Projeto

- **Linhas de CÃ³digo:** ~500
- **Arquivos de ConfiguraÃ§Ã£o:** 30+
- **Containers:** 5
- **Namespaces Kubernetes:** 3
- **MÃ©tricas Coletadas:** 50+
- **Alertas Configurados:** 3
- **DocumentaÃ§Ã£o:** 100+ pÃ¡ginas

---
- [x] Provisionar infraestrutura AWS
---
### âœ… Desafio 6 - Infraestrutura AWS
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- Infraestrutura completa provisionada na AWS usando Terraform
- Arquitetura modular com remote state isolado por componente
- Multi-AZ para alta disponibilidade
- RegiÃ£o: us-east-2 (Ohio)

**Recursos Provisionados:**

**Networking:**
- VPC 10.100.0.0/16 com 2 Availability Zones (us-east-2a, us-east-2b)
- 2 Subnets pÃºblicas + 2 Subnets privadas
- 2 NAT Gateways para alta disponibilidade
- Internet Gateway
- Route Tables configuradas

**Compute:**
- EKS Cluster v1.34 com control plane gerenciado
- 3 Node Groups:
  - spot_1: 2 nodes t3.medium/t3a.medium (SPOT)
  - spot_2: 2 nodes t3.large/t3a.large (SPOT)
  - on_demand: 1 node t3.medium (ON_DEMAND)
- Auto-scaling configurado (5-11 nodes)
- IAM Roles com polÃ­ticas de menor privilÃ©gio

**Databases:**
- RDS PostgreSQL 17.6 (db.t3.micro)
- Multi-AZ para failover automÃ¡tico
- Backup automatizado
- Criptografia at-rest habilitada

**Cache:**
- ElastiCache Redis 7.0 (cache.t3.micro)
- ReplicaÃ§Ã£o Multi-AZ
- Automatic failover habilitado

**Messaging:**
- MSK (Managed Kafka) 3.5.1
- 2 brokers kafka.t3.small
- Multi-AZ deployment
- Criptografia in-transit e at-rest

**Search & Analytics:**
- OpenSearch 2.11 (t3.small.search)
- 2 nodes para alta disponibilidade
- Fine-grained access control
- Encryption at-rest e node-to-node

**Container Registry:**
- ECR (Elastic Container Registry)
- Image scanning habilitado
- Lifecycle policy (manter Ãºltimas 10 imagens)

**SeguranÃ§a:**
- Security Groups isolados por serviÃ§o
- Criptografia habilitada em todos os recursos
- IAM Roles com polÃ­ticas especÃ­ficas
- Secrets gerenciados pela AWS
- VPC endpoints para serviÃ§os AWS

**Observabilidade:**
- EKS Control Plane Logs habilitados
- CloudWatch Logs integrado
- Tags padronizadas em todos os recursos

**Infraestrutura como CÃ³digo:**
- 10 mÃ³dulos Terraform independentes
- Remote state no S3 com versionamento
- Backend isolado por mÃ³dulo
- VariÃ¡veis centralizadas
- Tags padrÃ£o aplicadas automaticamente

**MÃ³dulos Terraform:**
```
00-s3_remote_state/    # Bucket S3 para remote state
01-vpc/                # VPC + Subnets + NAT Gateways
02-security_group/     # Security Groups isolados
03-iam/                # IAM Roles para EKS
04-eks/                # EKS Cluster v1.34
05-node_groups/        # 3 Node Groups (2 SPOT + 1 ON_DEMAND)
06-rds/                # PostgreSQL Multi-AZ
07-kafka/              # MSK (Kafka) 2 brokers
08-redis/              # ElastiCache Redis replicado
09-opensearch/         # OpenSearch 2 nodes
10-ecr/                # Container Registry
```

**Desafios TÃ©cnicos Superados:**
1. **Capacidade SPOT:** InstÃ¢ncias t3a.medium indisponÃ­veis em us-east-1a â†’ MigraÃ§Ã£o para us-east-2 (us-east-2a, us-east-2b)
2. **Naming OpenSearch:** Domain name com underscores invÃ¡lido â†’ ImplementaÃ§Ã£o de funÃ§Ã£o replace() para sanitizaÃ§Ã£o
3. **Tags Padronizadas:** RepetiÃ§Ã£o manual de tags â†’ ImplementaÃ§Ã£o de default_tags no provider AWS
4. **ModularizaÃ§Ã£o:** DependÃªncias entre mÃ³dulos â†’ Remote state data sources para comunicaÃ§Ã£o

**Boas PrÃ¡ticas Implementadas:**
- âœ… Infraestrutura modular e reutilizÃ¡vel
- âœ… Remote state isolado por componente
- âœ… Multi-AZ em todos os serviÃ§os crÃ­ticos
- âœ… Criptografia por padrÃ£o
- âœ… Security Groups com menor privilÃ©gio
- âœ… Auto-scaling configurado
- âœ… Backup automatizado
- âœ… Tags consistentes para governanÃ§a
- âœ… DocumentaÃ§Ã£o inline nos mÃ³dulos

**Custo Estimado Mensal:**
- VPC (NAT Gateways): ~$65
- EKS Control Plane: ~$73
- EC2 Nodes (5-11 instances): ~$80-120
- RDS PostgreSQL: ~$30
- MSK (Kafka): ~$150
- ElastiCache Redis: ~$25
- OpenSearch: ~$80
- **Total:** ~$500-550/mÃªs

**LocalizaÃ§Ã£o:** `terraform/SegundaSemana/`

**Tempo de Provisionamento:** ~45 minutos

**ValidaÃ§Ã£o:**
```bash
# Configurar kubectl
aws eks update-kubeconfig --name desafio-sre-junior-eks --region us-east-2

# Verificar nodes
kubectl get nodes

# Verificar recursos AWS
aws eks describe-cluster --name desafio-sre-junior-eks --region us-east-2
aws rds describe-db-instances --region us-east-2
aws elasticache describe-cache-clusters --region us-east-2
```

---
### âœ… Desafio 7 - CI/CD com ArgoCD
**Status:** ConcluÃ­do

**ImplementaÃ§Ã£o:**
- Pipeline CI/CD completo com GitHub Actions e ArgoCD
- GitOps deployment automatizado
- Build e push automÃ¡tico de imagens Docker
- SincronizaÃ§Ã£o automÃ¡tica de manifests Kubernetes
- AplicaÃ§Ã£o rodando em EKS com 3 rÃ©plicas

**Componentes:**

**GitHub Actions:**
- Workflow: `.github/workflows/build-deploy.yml`
- Trigger: Push em `main` com mudanÃ§as em `app/**`
- Build de imagem Docker otimizada
- Push para Docker Hub: `crfjunior65/flask-app:latest`
- Tempo de execuÃ§Ã£o: ~2-3 minutos

**ArgoCD:**
- Instalado no namespace `argocd`
- Application: `desafio-sre-app`
- Source: RepositÃ³rio Git (branch `main`)
- Path: `terraform/SegundaSemana/k8s-manifests/`
- Sync Policy: AutomÃ¡tico
- Self-Heal: Habilitado
- Prune: Habilitado

**Manifests Kubernetes:**
- **Deployment:** 3 rÃ©plicas Flask com health checks
- **Service:** LoadBalancer para acesso externo
- **ConfigMap:** Endpoints RDS, Redis, Kafka, OpenSearch
- **Secret:** Senha do PostgreSQL

**Fluxo CI/CD:**
```
1. Developer push cÃ³digo â†’ GitHub
2. GitHub Actions detecta mudanÃ§a em app/**
3. Build da imagem Docker
4. Push para Docker Hub (crfjunior65/flask-app:latest)
5. ArgoCD detecta mudanÃ§a no Git
6. ArgoCD sincroniza manifests com EKS
7. Kubernetes faz rolling update dos pods
8. LoadBalancer roteia trÃ¡fego para novos pods
```

**Recursos Deployados:**
```
NAMESPACE       RECURSO                 REPLICAS    STATUS
desafio-sre     flask-app               3/3         Running
desafio-sre     flask-app-service       1           LoadBalancer
desafio-sre     flask-config            1           ConfigMap
desafio-sre     postgres-secret         1           Secret
```

**Endpoints da AplicaÃ§Ã£o:**
- `/` - Status da aplicaÃ§Ã£o
- `/health` - Health check
- `/version` - VersÃ£o e deployed_by
- `/redis` - Teste conexÃ£o Redis
- `/postgres` - Teste conexÃ£o PostgreSQL
- `/metrics` - MÃ©tricas Prometheus

**Tempo de Deploy Completo:**
- GitHub Actions: 2-3 minutos
- ArgoCD Sync: 1-2 minutos
- Kubernetes Rollout: 1-2 minutos
- **Total:** 4-7 minutos

**ValidaÃ§Ã£o:**
```bash
# Verificar ArgoCD Application
kubectl get application -n argocd desafio-sre-app

# Verificar pods
kubectl get pods -n desafio-sre

# Obter URL do LoadBalancer
kubectl get svc -n desafio-sre flask-app-service

# Testar endpoints
curl http://<LOAD_BALANCER_URL>/health
curl http://<LOAD_BALANCER_URL>/version
```

**LocalizaÃ§Ã£o:**
- Workflow: `.github/workflows/build-deploy.yml`
- ArgoCD App: `terraform/SegundaSemana/argocd-application.yaml`
- Manifests: `terraform/SegundaSemana/k8s-manifests/`

**Desafios Enfrentados:**

1. **Symlinks no RepositÃ³rio Git**
   - **Problema:** ArgoCD bloqueou sync devido a symlinks de venv Python
   - **Erro:** "Illegal filepath in repo"
   - **SoluÃ§Ã£o:** Remover diretÃ³rios com symlinks do Git usando `git rm -r --cached`
   - **Aprendizado:** Python venv deve estar sempre no `.gitignore`

2. **Nome Incorreto da Imagem Docker**
   - **Problema:** Workflow usava `desafio-sre-app` mas deveria ser `flask-app`
   - **Impacto:** Imagens sendo enviadas para repositÃ³rio errado no Docker Hub
   - **SoluÃ§Ã£o:** Corrigir `IMAGE_NAME` no workflow e deployment manifest
   - **Aprendizado:** Padronizar nomes desde o inÃ­cio do projeto

3. **FunÃ§Ã£o Duplicada no CÃ³digo**
   - **Problema:** Duas funÃ§Ãµes com nome `version()` causando `AssertionError`
   - **Erro:** "View function mapping is overwriting an existing endpoint"
   - **SoluÃ§Ã£o:** Renomear segunda funÃ§Ã£o para `testes()`
   - **Aprendizado:** Validar cÃ³digo localmente antes de push

4. **Cache do ArgoCD Repo Server**
   - **Problema:** ArgoCD mantinha cache do repositÃ³rio com symlinks
   - **SoluÃ§Ã£o:** Restart do pod `argocd-repo-server` para limpar cache
   - **Comando:** `kubectl delete pod -n argocd -l app.kubernetes.io/name=argocd-repo-server`

5. **ConfiguraÃ§Ã£o de Secrets**
   - **Problema:** Secret do PostgreSQL nÃ£o criado automaticamente
   - **SoluÃ§Ã£o:** Criar manualmente via kubectl ou incluir no ArgoCD
   - **Aprendizado:** Secrets sensÃ­veis devem ser gerenciados separadamente

**Boas PrÃ¡ticas Implementadas:**
- âœ… GitOps: Manifests versionados no Git
- âœ… Imagens imutÃ¡veis: Tag com SHA do commit
- âœ… Health checks: Liveness e readiness probes
- âœ… Rolling updates: Zero downtime deployments
- âœ… Self-healing: ArgoCD reverte mudanÃ§as manuais
- âœ… Observabilidade: MÃ©tricas Prometheus expostas
- âœ… Secrets management: Separado do cÃ³digo

**Melhorias Futuras:**
- [ ] Implementar tags semÃ¢nticas (v1.0.0) ao invÃ©s de `latest`
- [ ] Adicionar testes automatizados no pipeline
- [ ] Implementar ambientes (dev, staging, prod)
- [ ] Configurar notificaÃ§Ãµes do ArgoCD (Slack/Email)
- [ ] Adicionar anÃ¡lise de seguranÃ§a de imagens (Trivy)

---

### Segunda Semana
- [ ] Deploy com ArgoCD
- [ ] Implementar APM
- [ ] Centralizar logs no OpenSearch
- [ ] DocumentaÃ§Ã£o completa

---

---

## ğŸ‘¤ Autor

**Junior Fernandes**  
SRE / DevOps - ElvenWorks

---

**Ãšltima atualizaÃ§Ã£o:** 04/12/2024  
**VersÃ£o:** 1.2  
**Status:** Desafio 7 ConcluÃ­do âœ…
