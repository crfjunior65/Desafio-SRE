Name:             flask-app-6d74c5d4f6-4krbp
Namespace:        desafio-sre
Priority:         0
Service Account:  default
Node:             app-cluster-control-plane/172.18.0.2
Start Time:       Tue, 02 Dec 2025 11:26:41 -0300
Labels:           app=flask-app
                  pod-template-hash=6d74c5d4f6
Annotations:      prometheus.io/path: /metrics
                  prometheus.io/port: 9999
                  prometheus.io/scrape: true
Status:           Pending
IP:               10.244.0.6
IPs:
  IP:           10.244.0.6
Controlled By:  ReplicaSet/flask-app-6d74c5d4f6
Containers:
  flask-app:
    Container ID:
    Image:          flask-app:v1.0
    Image ID:
    Ports:          5000/TCP, 9999/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Waiting
      Reason:       ErrImageNeverPull
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     300m
      memory:  256Mi
    Requests:
      cpu:      100m
      memory:   128Mi
    Liveness:   http-get http://:5000/ delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:5000/ delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:
      REDIS_HOST:     <set to the key 'REDIS_HOST' of config map 'app-config'>     Optional: false
      POSTGRES_HOST:  <set to the key 'POSTGRES_HOST' of config map 'app-config'>  Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-knkk5 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  kube-api-access-knkk5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason             Age                    From               Message
  ----     ------             ----                   ----               -------
  Normal   Scheduled          47m                    default-scheduler  Successfully assigned desafio-sre/flask-app-6d74c5d4f6-4krbp to app-cluster-control-plane
  Warning  Failed             45m (x12 over 47m)     kubelet            Error: ErrImageNeverPull
  Warning  ErrImageNeverPull  2m19s (x213 over 47m)  kubelet            Container image "flask-app:v1.0" is not present with pull policy of Never


Name:             flask-app-6d74c5d4f6-lflzk
Namespace:        desafio-sre
Priority:         0
Service Account:  default
Node:             app-cluster-control-plane/172.18.0.2
Start Time:       Tue, 02 Dec 2025 11:26:41 -0300
Labels:           app=flask-app
                  pod-template-hash=6d74c5d4f6
Annotations:      prometheus.io/path: /metrics
                  prometheus.io/port: 9999
                  prometheus.io/scrape: true
Status:           Pending
IP:               10.244.0.7
IPs:
  IP:           10.244.0.7
Controlled By:  ReplicaSet/flask-app-6d74c5d4f6
Containers:
  flask-app:
    Container ID:
    Image:          flask-app:v1.0
    Image ID:
    Ports:          5000/TCP, 9999/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Waiting
      Reason:       ErrImageNeverPull
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     300m
      memory:  256Mi
    Requests:
      cpu:      100m
      memory:   128Mi
    Liveness:   http-get http://:5000/ delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:5000/ delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:
      REDIS_HOST:     <set to the key 'REDIS_HOST' of config map 'app-config'>     Optional: false
      POSTGRES_HOST:  <set to the key 'POSTGRES_HOST' of config map 'app-config'>  Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-l9q8f (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  kube-api-access-l9q8f:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason             Age                    From               Message
  ----     ------             ----                   ----               -------
  Normal   Scheduled          47m                    default-scheduler  Successfully assigned desafio-sre/flask-app-6d74c5d4f6-lflzk to app-cluster-control-plane
  Warning  Failed             45m (x12 over 47m)     kubelet            Error: ErrImageNeverPull
  Warning  ErrImageNeverPull  2m26s (x210 over 47m)  kubelet            Container image "flask-app:v1.0" is not present with pull policy of Never


Name:             flask-app-6d74c5d4f6-nszdg
Namespace:        desafio-sre
Priority:         0
Service Account:  default
Node:             app-cluster-control-plane/172.18.0.2
Start Time:       Tue, 02 Dec 2025 11:26:41 -0300
Labels:           app=flask-app
                  pod-template-hash=6d74c5d4f6
Annotations:      prometheus.io/path: /metrics
                  prometheus.io/port: 9999
                  prometheus.io/scrape: true
Status:           Pending
IP:               10.244.0.5
IPs:
  IP:           10.244.0.5
Controlled By:  ReplicaSet/flask-app-6d74c5d4f6
Containers:
  flask-app:
    Container ID:
    Image:          flask-app:v1.0
    Image ID:
    Ports:          5000/TCP, 9999/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Waiting
      Reason:       ErrImageNeverPull
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     300m
      memory:  256Mi
    Requests:
      cpu:      100m
      memory:   128Mi
    Liveness:   http-get http://:5000/ delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:5000/ delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:
      REDIS_HOST:     <set to the key 'REDIS_HOST' of config map 'app-config'>     Optional: false
      POSTGRES_HOST:  <set to the key 'POSTGRES_HOST' of config map 'app-config'>  Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5b9wc (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  kube-api-access-5b9wc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason             Age                    From               Message
  ----     ------             ----                   ----               -------
  Normal   Scheduled          47m                    default-scheduler  Successfully assigned desafio-sre/flask-app-6d74c5d4f6-nszdg to app-cluster-control-plane
  Warning  Failed             45m (x12 over 47m)     kubelet            Error: ErrImageNeverPull
  Warning  ErrImageNeverPull  2m23s (x209 over 47m)  kubelet            Container image "flask-app:v1.0" is not present with pull policy of Never


Name:             flask-app-78cc5bb98f-zqhrr
Namespace:        desafio-sre
Priority:         0
Service Account:  default
Node:             app-cluster-control-plane/172.18.0.2
Start Time:       Tue, 02 Dec 2025 12:10:45 -0300
Labels:           app=flask-app
                  pod-template-hash=78cc5bb98f
Annotations:      prometheus.io/path: /metrics
                  prometheus.io/port: 9999
                  prometheus.io/scrape: true
Status:           Pending
IP:               10.244.0.11
IPs:
  IP:           10.244.0.11
Controlled By:  ReplicaSet/flask-app-78cc5bb98f
Containers:
  flask-app:
    Container ID:
    Image:          flask-app:v1.0
    Image ID:
    Ports:          5000/TCP, 9999/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     300m
      memory:  256Mi
    Requests:
      cpu:      100m
      memory:   128Mi
    Liveness:   http-get http://:5000/ delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:5000/ delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:
      REDIS_HOST:     <set to the key 'REDIS_HOST' of config map 'app-config'>     Optional: false
      POSTGRES_HOST:  <set to the key 'POSTGRES_HOST' of config map 'app-config'>  Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-s5s7j (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  kube-api-access-s5s7j:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  3m28s                 default-scheduler  Successfully assigned desafio-sre/flask-app-78cc5bb98f-zqhrr to app-cluster-control-plane
  Normal   Pulling    41s (x5 over 3m27s)   kubelet            Pulling image "flask-app:v1.0"
  Warning  Failed     40s (x5 over 3m26s)   kubelet            Failed to pull image "flask-app:v1.0": failed to pull and unpack image "docker.io/library/flask-app:v1.0": failed to resolve reference "docker.io/library/flask-app:v1.0": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed
  Warning  Failed     40s (x5 over 3m26s)   kubelet            Error: ErrImagePull
  Normal   BackOff    11s (x12 over 3m26s)  kubelet            Back-off pulling image "flask-app:v1.0"
  Warning  Failed     11s (x12 over 3m26s)  kubelet            Error: ImagePullBackOff


Name:             postgres-6856b9754d-l4tlv
Namespace:        desafio-sre
Priority:         0
Service Account:  default
Node:             app-cluster-control-plane/172.18.0.2
Start Time:       Tue, 02 Dec 2025 12:06:13 -0300
Labels:           app=postgres
                  pod-template-hash=6856b9754d
Annotations:      <none>
Status:           Running
IP:               10.244.0.10
IPs:
  IP:           10.244.0.10
Controlled By:  ReplicaSet/postgres-6856b9754d
Containers:
  postgres:
    Container ID:   containerd://fa907f59d8a694793a9683237a04570094d3723a5b248b73e3094ab29bea7622
    Image:          postgres:15-alpine
    Image ID:       docker.io/library/postgres@sha256:aa7b1ef595e165f0b780162e3a41edd0a7ed3ea672eb8a0f81615ba725e62bc5
    Port:           5432/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 02 Dec 2025 12:06:21 -0300
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:      200m
      memory:   256Mi
    Liveness:   exec [pg_isready -U postgres] delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  exec [pg_isready -U postgres] delay=10s timeout=1s period=5s #success=1 #failure=3
    Environment:
      POSTGRES_DB:        <set to the key 'POSTGRES_DB' of config map 'app-config'>    Optional: false
      POSTGRES_USER:      <set to the key 'POSTGRES_USER' of config map 'app-config'>  Optional: false
      POSTGRES_PASSWORD:  <set to the key 'POSTGRES_PASSWORD' in secret 'app-secret'>  Optional: false
    Mounts:
      /var/lib/postgresql/data from postgres-storage (rw,path="postgres")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-r8lgz (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       True
  ContainersReady             True
  PodScheduled                True
Volumes:
  postgres-storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  postgres-pvc
    ReadOnly:   false
  kube-api-access-r8lgz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  12m (x8 over 47m)  default-scheduler  0/1 nodes are available: persistentvolumeclaim "postgres-pvc" not found. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         8m                 default-scheduler  Successfully assigned desafio-sre/postgres-6856b9754d-l4tlv to app-cluster-control-plane
  Normal   Pulling           8m                 kubelet            Pulling image "postgres:15-alpine"
  Normal   Pulled            7m52s              kubelet            Successfully pulled image "postgres:15-alpine" in 8.353s (8.353s including waiting). Image size: 108698148 bytes.
  Normal   Created           7m52s              kubelet            Created container: postgres
  Normal   Started           7m52s              kubelet            Started container postgres


Name:             redis-5f9f6ff956-xzrtb
Namespace:        desafio-sre
Priority:         0
Service Account:  default
Node:             app-cluster-control-plane/172.18.0.2
Start Time:       Tue, 02 Dec 2025 11:26:41 -0300
Labels:           app=redis
                  pod-template-hash=5f9f6ff956
Annotations:      <none>
Status:           Running
IP:               10.244.0.8
IPs:
  IP:           10.244.0.8
Controlled By:  ReplicaSet/redis-5f9f6ff956
Containers:
  redis:
    Container ID:   containerd://64768f91f7e83ad30077a7cda27c7dd2c668b4953d4b2a099f4e06fe022488d2
    Image:          redis:7-alpine
    Image ID:       docker.io/library/redis@sha256:ee64a64eaab618d88051c3ade8f6352d11531fcf79d9a4818b9b183d8c1d18ba
    Port:           6379/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 02 Dec 2025 11:26:46 -0300
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     200m
      memory:  256Mi
    Requests:
      cpu:        100m
      memory:     128Mi
    Liveness:     exec [redis-cli ping] delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:    exec [redis-cli ping] delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-prqf4 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       True
  ContainersReady             True
  PodScheduled                True
Volumes:
  kube-api-access-prqf4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  47m   default-scheduler  Successfully assigned desafio-sre/redis-5f9f6ff956-xzrtb to app-cluster-control-plane
  Normal  Pulling    47m   kubelet            Pulling image "redis:7-alpine"
  Normal  Pulled     47m   kubelet            Successfully pulled image "redis:7-alpine" in 4.705s (4.705s including waiting). Image size: 17253204 bytes.
  Normal  Created    47m   kubelet            Created container: redis
  Normal  Started    47m   kubelet            Started container redis
